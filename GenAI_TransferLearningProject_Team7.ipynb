{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52016/GEN-AI/blob/main/GenAI_TransferLearningProject_Team7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLMyYmosihBh"
      },
      "source": [
        "#**Importing tensorflow package**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NjzH-eqWH0Q2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c5Sxi74wJomY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fJct3cqio5e"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#**Mounting Drive and Creating directories for the Images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVkuPJk1JwEl",
        "outputId": "806fed06-4eb0-4b45-9a41-78508ba601fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTDu0LZjJ0NO",
        "outputId": "f324efe0-ecc2-4b25-dfe3-0eb9c0e0f2cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4000 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "data_dir = '/content/drive/MyDrive/DogEmotions/Images/training_set'  # Path to the training dataset\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalize pixel values to [0, 1]\n",
        "    shear_range=0.2,  # Apply random shear transformation\n",
        "    zoom_range=0.2,  # Apply random zoom\n",
        "    horizontal_flip=True  # Randomly flip images horizontally\n",
        ")\n",
        "\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "    data_dir,  # Directory containing class-wise image folders\n",
        "    target_size=(64, 64),  # Resize all images to 64x64 pixels\n",
        "    batch_size=32,  # Number of images per batch\n",
        "    class_mode='categorical'  # Use binary labels (0 or 1) for two classes\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BPHVDMPJ72n",
        "outputId": "88d0dea8-cbda-4d38-a40f-1f1e9c2ae51f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 800 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "test_dir = '/content/drive/MyDrive/DogEmotions/Images/test_set'  # Path to the test dataset\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)  # Only normalize, no augmentation for test data\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "    test_dir,  # Directory containing test images in class-wise folders\n",
        "    target_size=(64, 64),  # Resize all images to 64x64 pixels\n",
        "    batch_size=32,  # Number of images per batch\n",
        "    class_mode='categorical'  # Binary classification (0 or 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nLU8zgDiyYO"
      },
      "source": [
        "#**Building the MobileNetV2 Model**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2  # Import MobileNetV2"
      ],
      "metadata": {
        "id": "VO7nZ2iaQBty"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TGh86-Ha_q-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3faad5b-e9b4-4010-f796-7799d60bcd4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-315a82fbcfc2>:1: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(input_shape=(64, 64, 3), include_top=False, weights='imagenet')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "base_model = MobileNetV2(input_shape=(64, 64, 3), include_top=False, weights='imagenet')\n",
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using GlobalAveragePooling2D() to Flattening the 3D Model**"
      ],
      "metadata": {
        "id": "Asr3WPEaUOcG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "K0UPjUoA4xJU"
      },
      "outputs": [],
      "source": [
        "x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9MX_mRqjEcF"
      },
      "source": [
        "###**Adding 1st Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xy2nAvvE6FE2"
      },
      "outputs": [],
      "source": [
        "x = tf.keras.layers.Dense(256, activation='relu')(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaLMUV2fjN99"
      },
      "source": [
        "###**Adding Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qcx9HqpY66Ft"
      },
      "outputs": [],
      "source": [
        "x = tf.keras.layers.Dense(128, activation='relu')(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VogUIo7jVrD"
      },
      "source": [
        "###**Adding Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7gDLk_0xVCkQ"
      },
      "outputs": [],
      "source": [
        "x = tf.keras.layers.Dense(256, activation='relu')(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyAGD-Z9jdsb"
      },
      "source": [
        "###**Adding a output layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aQkf3Mrg8Hst"
      },
      "outputs": [],
      "source": [
        "output = tf.keras.layers.Dense(4, activation='softmax')(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTZSspPwju2Y"
      },
      "source": [
        "###**Compiling and fitting the value**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9ZZRonnj8V_B"
      },
      "outputs": [],
      "source": [
        "mobilenetv2 = tf.keras.Model(inputs=base_model.input, outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zVGZpla59TbK"
      },
      "outputs": [],
      "source": [
        "mobilenetv2.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenetv2.fit(\n",
        "    training_set,\n",
        "    epochs=10,\n",
        "    validation_data=test_set\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhEo9r2sSytC",
        "outputId": "379b2764-07e8-470d-b9e8-a2040ba8b927"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1278s\u001b[0m 10s/step - accuracy: 0.3275 - loss: 1.4208 - val_accuracy: 0.3537 - val_loss: 1.3458\n",
            "Epoch 2/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 341ms/step - accuracy: 0.4012 - loss: 1.2964 - val_accuracy: 0.3713 - val_loss: 1.3486\n",
            "Epoch 3/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 333ms/step - accuracy: 0.4301 - loss: 1.2370 - val_accuracy: 0.3800 - val_loss: 1.3541\n",
            "Epoch 4/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 339ms/step - accuracy: 0.4578 - loss: 1.1938 - val_accuracy: 0.4000 - val_loss: 1.3326\n",
            "Epoch 5/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 340ms/step - accuracy: 0.4908 - loss: 1.1478 - val_accuracy: 0.3850 - val_loss: 1.3256\n",
            "Epoch 6/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 340ms/step - accuracy: 0.5287 - loss: 1.0990 - val_accuracy: 0.3725 - val_loss: 1.3623\n",
            "Epoch 7/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 345ms/step - accuracy: 0.5505 - loss: 1.0441 - val_accuracy: 0.3725 - val_loss: 1.4213\n",
            "Epoch 8/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 336ms/step - accuracy: 0.5550 - loss: 1.0170 - val_accuracy: 0.3713 - val_loss: 1.4283\n",
            "Epoch 9/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 335ms/step - accuracy: 0.5847 - loss: 0.9954 - val_accuracy: 0.3713 - val_loss: 1.4891\n",
            "Epoch 10/10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 370ms/step - accuracy: 0.6119 - loss: 0.9424 - val_accuracy: 0.3450 - val_loss: 1.4921\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7eb79a1ea510>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7Kj4Xxei7kX"
      },
      "source": [
        "#**Predicting the answer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zI30nGvzO-iK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "test_image = image.load_img(\n",
        "    '/content/drive/MyDrive/Deep_Learning/DL/CNN/dataset/single_prediction/cat_or_dog_1.jpg',\n",
        "    target_size=(64, 64)\n",
        ")\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "test_image = test_image / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = mobilenetv2.predict(test_image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsEuxGjHTTB5",
        "outputId": "459ed5fb-350f-4464-ec47-50eb12bca4d6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['angry', 'happy', 'relaxed', 'sad']"
      ],
      "metadata": {
        "id": "bAdQo6VBTWZR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_class = class_names[np.argmax(result)]"
      ],
      "metadata": {
        "id": "dLGAgLotTcCw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted Emotion:\", predicted_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ62SDu4TgzM",
        "outputId": "9eaac2a0-5f1a-4834-c7ba-fcf6e3e5d0f0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Emotion: relaxed\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKm5MfefpXoHTkT+Kf/Zbt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}